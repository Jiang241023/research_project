out_dir: results
metric_best: mae # use validation MAE (mean absolute error) to pick the best epoch
metric_agg: argmin # Use argmin for lower-is-better metrics (MAE, MSE, RMSE, CE/NLL).
accelerator: "cuda:0"
tensorboard_each_run: True

dataset:
  format: PyG-DDACSNPYStream     
  name: node-regression
  dir: /mnt/data/jiang          
  task: graph
  split_mode: standard
  task_type: regression
  transductive: False
  node_encoder: True
  node_encoder_name: LinearNode
  node_encoder_bn: False
  edge_encoder: True
  edge_encoder_name: LinearEdge
  edge_encoder_bn: False
  max_samples: None

posenc_RRWP:
  enable: True
  ksteps: 5
  add_identity: True
  add_node_attr: False
  add_inverse: False

train:
  mode: train
  batch_size: 16              
  eval_period: 1
  ckpt_best: True
  ckpt_clean: True

model:
  type: GritTransformer
  # loss_fun: mse
  loss_fun: mse_combined_with_laplacian
  alpha: 0.8
  beta: 0.2        
  laplace_normalized: True     # normalized Laplacian for degree-robustness
  laplace_norm_mode: per_node  # keeps magnitude stable across graphs
  laplace_on_residuals: True   # set true to smooth errors instead of predictions                
  only_mse: False             # !!! if set it to be true, the loss function will become mse!!!!

gt:
  layer_type: GritTransformer
  layers: 3
  n_heads: 2
  dim_hidden: 64
  dropout: 0.1
  attn_dropout: 0.1
  layer_norm: False
  batch_norm: True
  update_e: True
  attn:
    clamp: 5.
    act: 'relu'
    full_attn: True
    edge_enhance: True
    O_e: True
    norm_e: True
    signed_sqrt: True

gnn:
  head: inductive_node
  layers_pre_mp: 0  # 'pre–message passing' MLP
  layers_post_mp: 3 # 'post–message passing' MLP
  dim_inner: 64
  dim_out: 3
  batchnorm: True
  act: relu
  dropout: 0.0

optim:
  clip_grad_norm: True
  optimizer: adamW
  weight_decay: 1e-4
  base_lr: 0.0005
  max_epoch: 100
  scheduler: cosine_with_warmup
  num_warmup_epochs: 15
